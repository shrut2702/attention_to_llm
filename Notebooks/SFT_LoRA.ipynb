{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "by9abSsN50rC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import tiktoken as tk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/LLM/Notebooks')"
      ],
      "metadata": {
        "id": "Z22qPMLX59Ug"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "from pathlib import Path\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from functools import partial\n",
        "import math"
      ],
      "metadata": {
        "id": "BPXgP5Nt59Xk"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gpt_download import download_and_load_gpt2\n",
        "from gpt2 import *\n",
        "from utils import *"
      ],
      "metadata": {
        "id": "lo2vGYTM59a8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preparation**"
      ],
      "metadata": {
        "id": "sYmDQ99n6Egw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_and_load_file(file_path, url):\n",
        "  if not os.path.exists(file_path):\n",
        "    with urllib.request.urlopen(url) as response:\n",
        "      text_data = response.read().decode(\"utf-8\")\n",
        "    with open(file_path, 'w', encoding='utf-8') as f:\n",
        "      f.write(text_data)\n",
        "  else:\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "      text_data = f.read()\n",
        "  with open(file_path, 'r') as f:\n",
        "    data = json.load(f)\n",
        "  return data"
      ],
      "metadata": {
        "id": "34lC_pOs59eF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/My Drive/LLM/Data/instruction-data.json'\n",
        "url = 'https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/instruction-data.json'\n",
        "\n",
        "data = download_and_load_file(file_path, url)\n",
        "print(\"Total entries: \", len(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFt2b2HM59ht",
        "outputId": "1ed41e6a-d01d-4d3e-b78d-0f00124ddf92"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total entries:  1100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(data, train_size, test_size):\n",
        "  train_data = data[:int(len(data) * train_size)]\n",
        "  test_data = data[int(len(data) * train_size) : int(len(data) * (train_size + test_size))]\n",
        "  val_data = data[int(len(data) * (train_size + test_size)):]\n",
        "\n",
        "  return train_data, val_data, test_data"
      ],
      "metadata": {
        "id": "hnCM8peF59kv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, val_data, test_data = split_data(data, 0.85, 0.1)"
      ],
      "metadata": {
        "id": "yJptg70C59n-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_entry(entry):\n",
        "  instruction_text = (\n",
        "      f\"Below is the instruction that describes a task. Write an appropriate response to address the instruction. \"\n",
        "      f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
        "  )\n",
        "\n",
        "  input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry['input'] else \"\"\n",
        "\n",
        "  return instruction_text + input_text"
      ],
      "metadata": {
        "id": "NOKSaLIw6MGs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InstructionDataset(Dataset):\n",
        "  def __init__(self, data, tokenizer):\n",
        "    self.data = data\n",
        "    self.entry_lst = []\n",
        "\n",
        "    for entry in self.data:\n",
        "      instruction_text = format_entry(entry)\n",
        "      response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
        "      text = instruction_text + response_text\n",
        "\n",
        "      encoded_text = tokenizer.encode(text)\n",
        "      self.entry_lst.append(encoded_text)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.entry_lst[idx]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.entry_lst)"
      ],
      "metadata": {
        "id": "rUL9J78i6MJv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tk.get_encoding('gpt2')"
      ],
      "metadata": {
        "id": "2Gl4E0FC6MMk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_fn(batch, pad_token_id = 50256, ignore_index=-100, allowed_max_length=1024, device='cpu'):\n",
        "  batch_max_length = max([len(item)+1 for item in batch])\n",
        "  inputs_lst, targets_lst = [], []\n",
        "\n",
        "  for item in batch:\n",
        "    new_item = item.copy()\n",
        "    new_item += [pad_token_id]\n",
        "\n",
        "    padded_item = new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
        "\n",
        "    input = padded_item[:-1]\n",
        "    target = padded_item[1:]\n",
        "\n",
        "    input = torch.tensor(input)\n",
        "    target = torch.tensor(target)\n",
        "\n",
        "    mask = target == pad_token_id\n",
        "    indices = torch.nonzero(mask).squeeze()\n",
        "    if indices.numel() > 1:\n",
        "      target[indices[1:]] = ignore_index\n",
        "\n",
        "    if allowed_max_length is not None:\n",
        "      input = input[:allowed_max_length]\n",
        "      target = target[:allowed_max_length]\n",
        "\n",
        "    inputs_lst.append(input)\n",
        "    targets_lst.append(target)\n",
        "\n",
        "  return torch.stack(inputs_lst).to(device), torch.stack(targets_lst).to(device)\n"
      ],
      "metadata": {
        "id": "wmLcpU7J6xyX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "HxSuFyHL7IYY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customized_collate_fn = partial(custom_collate_fn, pad_token_id = 50256, ignore_index=-100, allowed_max_length=1024, device=device)"
      ],
      "metadata": {
        "id": "neVqjJK27Jx_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_workers = 0\n",
        "batch_size = 8\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_dataset = InstructionDataset(train_data, tokenizer)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, collate_fn=customized_collate_fn, num_workers=num_workers)\n",
        "\n",
        "val_dataset = InstructionDataset(val_data, tokenizer)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=False, collate_fn=customized_collate_fn, num_workers=num_workers)\n",
        "\n",
        "test_dataset = InstructionDataset(test_data, tokenizer)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False, collate_fn=customized_collate_fn, num_workers=num_workers)"
      ],
      "metadata": {
        "id": "55szzUaw7J1E"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x,y in train_loader:\n",
        "  print(x.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zpDcy6KpJ789",
        "outputId": "70ac19b0-83a2-49c8-b5c4-7225fcfde9e4"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 58]) torch.Size([8, 58])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 81]) torch.Size([8, 81])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 60]) torch.Size([8, 60])\n",
            "torch.Size([8, 89]) torch.Size([8, 89])\n",
            "torch.Size([8, 58]) torch.Size([8, 58])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 73]) torch.Size([8, 73])\n",
            "torch.Size([8, 59]) torch.Size([8, 59])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 92]) torch.Size([8, 92])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 81]) torch.Size([8, 81])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 82]) torch.Size([8, 82])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 78]) torch.Size([8, 78])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 84]) torch.Size([8, 84])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 79]) torch.Size([8, 79])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 84]) torch.Size([8, 84])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 73]) torch.Size([8, 73])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 88]) torch.Size([8, 88])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 73]) torch.Size([8, 73])\n",
            "torch.Size([8, 84]) torch.Size([8, 84])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 84]) torch.Size([8, 84])\n",
            "torch.Size([8, 90]) torch.Size([8, 90])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 84]) torch.Size([8, 84])\n",
            "torch.Size([8, 78]) torch.Size([8, 78])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 81]) torch.Size([8, 81])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 92]) torch.Size([8, 92])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 92]) torch.Size([8, 92])\n",
            "torch.Size([8, 81]) torch.Size([8, 81])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Pretrained weights**"
      ],
      "metadata": {
        "id": "5EG42vMg7PkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'gpt2-medium (355M)'\n",
        "model_size = model_name.split(' ')[-1].lstrip('(').rstrip(')')\n",
        "BASE_CONFIG.update(model_configs[model_name])"
      ],
      "metadata": {
        "id": "Gq1Kb-9B7PHx"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "settings, params = download_and_load_gpt2(model_size, models_dir='gpt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNyEmH2K7J4X",
        "outputId": "2400859f-16c9-4190-98a0-3e134ccb33e6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 172kiB/s]\n",
            "encoder.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 3.07MiB/s]\n",
            "hparams.json: 100%|██████████| 91.0/91.0 [00:00<00:00, 169kiB/s]\n",
            "model.ckpt.data-00000-of-00001: 100%|██████████| 1.42G/1.42G [01:36<00:00, 14.6MiB/s]\n",
            "model.ckpt.index: 100%|██████████| 10.4k/10.4k [00:00<00:00, 13.2MiB/s]\n",
            "model.ckpt.meta: 100%|██████████| 927k/927k [00:00<00:00, 2.64MiB/s]\n",
            "vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 1.51MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPTModel(BASE_CONFIG)\n",
        "load_parameters(model, params)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JJT7y3fe7J7F",
        "outputId": "68ca2553-8c79-466c-90ea-0f22dd121720"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (token_emb_layer): Embedding(50257, 1024)\n",
              "  (pos_emb_layer): Embedding(1024, 1024)\n",
              "  (dropout_layer): Dropout(p=0.0, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (mha): MultiHeadAttention(\n",
              "        (w_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GeLU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (mha): MultiHeadAttention(\n",
              "        (w_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GeLU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (mha): MultiHeadAttention(\n",
              "        (w_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GeLU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (mha): MultiHeadAttention(\n",
              "        (w_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GeLU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (mha): MultiHeadAttention(\n",
              "        (w_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GeLU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (mha): MultiHeadAttention(\n",
              "        (w_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GeLU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (mha): MultiHeadAttention(\n",
              "        (w_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GeLU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (mha): MultiHeadAttention(\n",
              "        (w_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GeLU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (mha): MultiHeadAttention(\n",
              "        (w_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GeLU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (mha): MultiHeadAttention(\n",
              "        (w_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GeLU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (mha): MultiHeadAttention(\n",
              "        (w_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GeLU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (mha): MultiHeadAttention(\n",
              "        (w_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GeLU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (12): TransformerBlock(\n",
              "      (mha): MultiHeadAttention(\n",
              "        (w_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GeLU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (13): TransformerBlock(\n",
              "      (mha): MultiHeadAttention(\n",
              "        (w_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GeLU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (14): TransformerBlock(\n",
              "      (mha): MultiHeadAttention(\n",
              "        (w_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GeLU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (15): TransformerBlock(\n",
              "      (mha): MultiHeadAttention(\n",
              "        (w_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GeLU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (16): TransformerBlock(\n",
              "      (mha): MultiHeadAttention(\n",
              "        (w_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GeLU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (17): TransformerBlock(\n",
              "      (mha): MultiHeadAttention(\n",
              "        (w_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GeLU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (18): TransformerBlock(\n",
              "      (mha): MultiHeadAttention(\n",
              "        (w_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GeLU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (19): TransformerBlock(\n",
              "      (mha): MultiHeadAttention(\n",
              "        (w_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GeLU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (20): TransformerBlock(\n",
              "      (mha): MultiHeadAttention(\n",
              "        (w_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GeLU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (21): TransformerBlock(\n",
              "      (mha): MultiHeadAttention(\n",
              "        (w_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GeLU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (22): TransformerBlock(\n",
              "      (mha): MultiHeadAttention(\n",
              "        (w_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GeLU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (23): TransformerBlock(\n",
              "      (mha): MultiHeadAttention(\n",
              "        (w_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (w_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GeLU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (output_layer): Linear(in_features=1024, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lets calculate loss before adding lora weights\n",
        "torch.manual_seed(123)\n",
        "model.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "  train_loss = calc_loss_dataloader(train_loader, model, device, 5)\n",
        "  val_loss = calc_loss_dataloader(val_loader, model, device, 5)\n",
        "\n",
        "print(f\"Training loss: {train_loss}\")\n",
        "print(f\"Validation loss : {val_loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGJS2xDuQ6qL",
        "outputId": "ab656847-9acf-4dad-a53f-14555dc2b7f5"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 3.8755919456481935\n",
            "Validation loss : 3.8087229251861574\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Add LoRA weights to Linear layers**"
      ],
      "metadata": {
        "id": "1oGQH0Jy7bX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LoRA(nn.Module):\n",
        "  def __init__(self, in_size, out_size, rank, alpha):\n",
        "    super().__init__()\n",
        "    self.alpha = alpha\n",
        "    self.A = nn.Parameter(torch.empty(in_size, rank))\n",
        "    nn.init.kaiming_uniform_(self.A, a=math.sqrt(5)) #weight initialization, this fn is called inside Linear module for weight initialization\n",
        "    self.B = nn.Parameter(torch.zeros(rank, out_size))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.alpha * (x @ self.A @ self.B)"
      ],
      "metadata": {
        "id": "pQ0Y0LdG7jQX"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#module to replace the linear layer\n",
        "class LinearWithLoRA(nn.Module):\n",
        "  def __init__(self, linear, rank, alpha):\n",
        "    super().__init__()\n",
        "    self.linear = linear\n",
        "    self.lora = LoRA(linear.in_features, linear.out_features, rank, alpha)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.linear(x) + self.lora(x)"
      ],
      "metadata": {
        "id": "5OQE7LfP9Yeu"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_linear_with_lora(model, alpha, rank):\n",
        "  for name, module in model.named_children():\n",
        "    if isinstance(module, nn.Linear):\n",
        "      setattr(model, name, LinearWithLoRA(module, rank, alpha))\n",
        "    else:\n",
        "      replace_linear_with_lora(module, rank, alpha)"
      ],
      "metadata": {
        "id": "GDgquuEN-a93"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rank=16\n",
        "alpha=16\n",
        "trainable_params_before = sum(param.numel() for param in model.parameters() if param.requires_grad)\n",
        "#lets freeze the weights\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False\n",
        "replace_linear_with_lora(model, rank=rank, alpha=alpha)\n",
        "trainable_params_after = sum(param.numel() for param in model.parameters() if param.requires_grad)\n",
        "\n",
        "print(f\"Trainable parameters before freezing pretrained weights: {trainable_params_before}\")\n",
        "print(f\"Trainable parameters after freezing the pretrained weights and adding LoRA weights: {trainable_params_after}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCTuj9qHHdOU",
        "outputId": "c5bc7182-2d94-4525-8391-76ed46dc2ee9"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable parameters before freezing pretrained weights: 406286336\n",
            "Trainable parameters after freezing the pretrained weights and adding LoRA weights: 7898384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fine-Tuning**"
      ],
      "metadata": {
        "id": "P0P6vySM7WU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#lets calculate loss before fine-tuning the model\n",
        "torch.manual_seed(123)\n",
        "model.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "  train_loss = calc_loss_dataloader(train_loader, model, device, 5)\n",
        "  val_loss = calc_loss_dataloader(val_loader, model, device, 5)\n",
        "\n",
        "print(f\"Training loss: {train_loss}\")\n",
        "print(f\"Validation loss : {val_loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOxod07-7Vzw",
        "outputId": "fefd3f03-5bac-43ad-f54a-73d9d93d782a"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 3.8755919456481935\n",
            "Validation loss : 3.8087229251861574\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#fine-tune\n",
        "torch.manual_seed(123)\n",
        "\n",
        "t0 = time.time()\n",
        "\n",
        "epochs = 2\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)\n",
        "\n",
        "train_losses, val_losses, track_tokens_seen = train_model_simple(model, train_loader, val_loader, device, optimizer, epochs, eval_freq=5, eval_iter=5, start_context=format_entry(val_data[0]), tokenizer=tokenizer)\n",
        "\n",
        "t1 = time.time()\n",
        "\n",
        "exec_time = (t1 - t0)/60 # in minutes\n",
        "\n",
        "print(f\"Training completed in {exec_time} mins\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLYf56R07T5s",
        "outputId": "269b3d2f-0108-46c4-c6ce-34ff301c79b3"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss after epoch 0 (Step: 0): 2.658\n",
            "Val loss after epoch 0 (Step: 0): 2.668\n",
            "Number of tokens seen after epoch 0 (Step: 0)\n",
            "Train loss after epoch 0 (Step: 5): 1.192\n",
            "Val loss after epoch 0 (Step: 5): 1.105\n",
            "Number of tokens seen after epoch 0 (Step: 5)\n",
            "Train loss after epoch 0 (Step: 10): 0.902\n",
            "Val loss after epoch 0 (Step: 10): 0.991\n",
            "Number of tokens seen after epoch 0 (Step: 10)\n",
            "Train loss after epoch 0 (Step: 15): 0.866\n",
            "Val loss after epoch 0 (Step: 15): 0.949\n",
            "Number of tokens seen after epoch 0 (Step: 15)\n",
            "Train loss after epoch 0 (Step: 20): 0.799\n",
            "Val loss after epoch 0 (Step: 20): 0.921\n",
            "Number of tokens seen after epoch 0 (Step: 20)\n",
            "Train loss after epoch 0 (Step: 25): 0.772\n",
            "Val loss after epoch 0 (Step: 25): 0.884\n",
            "Number of tokens seen after epoch 0 (Step: 25)\n",
            "Train loss after epoch 0 (Step: 30): 0.803\n",
            "Val loss after epoch 0 (Step: 30): 0.854\n",
            "Number of tokens seen after epoch 0 (Step: 30)\n",
            "Train loss after epoch 0 (Step: 35): 0.727\n",
            "Val loss after epoch 0 (Step: 35): 0.821\n",
            "Number of tokens seen after epoch 0 (Step: 35)\n",
            "Train loss after epoch 0 (Step: 40): 0.691\n",
            "Val loss after epoch 0 (Step: 40): 0.813\n",
            "Number of tokens seen after epoch 0 (Step: 40)\n",
            "Train loss after epoch 0 (Step: 45): 0.643\n",
            "Val loss after epoch 0 (Step: 45): 0.796\n",
            "Number of tokens seen after epoch 0 (Step: 45)\n",
            "Train loss after epoch 0 (Step: 50): 0.695\n",
            "Val loss after epoch 0 (Step: 50): 0.786\n",
            "Number of tokens seen after epoch 0 (Step: 50)\n",
            "Train loss after epoch 0 (Step: 55): 0.769\n",
            "Val loss after epoch 0 (Step: 55): 0.771\n",
            "Number of tokens seen after epoch 0 (Step: 55)\n",
            "Train loss after epoch 0 (Step: 60): 0.741\n",
            "Val loss after epoch 0 (Step: 60): 0.753\n",
            "Number of tokens seen after epoch 0 (Step: 60)\n",
            "Train loss after epoch 0 (Step: 65): 0.656\n",
            "Val loss after epoch 0 (Step: 65): 0.740\n",
            "Number of tokens seen after epoch 0 (Step: 65)\n",
            "Train loss after epoch 0 (Step: 70): 0.555\n",
            "Val loss after epoch 0 (Step: 70): 0.735\n",
            "Number of tokens seen after epoch 0 (Step: 70)\n",
            "Train loss after epoch 0 (Step: 75): 0.576\n",
            "Val loss after epoch 0 (Step: 75): 0.742\n",
            "Number of tokens seen after epoch 0 (Step: 75)\n",
            "Train loss after epoch 0 (Step: 80): 0.616\n",
            "Val loss after epoch 0 (Step: 80): 0.736\n",
            "Number of tokens seen after epoch 0 (Step: 80)\n",
            "Train loss after epoch 0 (Step: 85): 0.518\n",
            "Val loss after epoch 0 (Step: 85): 0.714\n",
            "Number of tokens seen after epoch 0 (Step: 85)\n",
            "Train loss after epoch 0 (Step: 90): 0.574\n",
            "Val loss after epoch 0 (Step: 90): 0.698\n",
            "Number of tokens seen after epoch 0 (Step: 90)\n",
            "Train loss after epoch 0 (Step: 95): 0.511\n",
            "Val loss after epoch 0 (Step: 95): 0.691\n",
            "Number of tokens seen after epoch 0 (Step: 95)\n",
            "Train loss after epoch 0 (Step: 100): 0.521\n",
            "Val loss after epoch 0 (Step: 100): 0.683\n",
            "Number of tokens seen after epoch 0 (Step: 100)\n",
            "Train loss after epoch 0 (Step: 105): 0.581\n",
            "Val loss after epoch 0 (Step: 105): 0.678\n",
            "Number of tokens seen after epoch 0 (Step: 105)\n",
            "Train loss after epoch 0 (Step: 110): 0.577\n",
            "Val loss after epoch 0 (Step: 110): 0.673\n",
            "Number of tokens seen after epoch 0 (Step: 110)\n",
            "Train loss after epoch 0 (Step: 115): 0.511\n",
            "Val loss after epoch 0 (Step: 115): 0.668\n",
            "Number of tokens seen after epoch 0 (Step: 115)\n",
            "Below is the instruction that describes a task. Write an appropriate response to address the instruction.   ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is prepared every day by the chef.<|endoftext|>The following is a list of movies that have been released in the United States.  ### Title: The Great Gatsby  ### Genre:\n",
            "Train loss after epoch 1 (Step: 120): 0.450\n",
            "Val loss after epoch 1 (Step: 120): 0.682\n",
            "Number of tokens seen after epoch 1 (Step: 120)\n",
            "Train loss after epoch 1 (Step: 125): 0.445\n",
            "Val loss after epoch 1 (Step: 125): 0.702\n",
            "Number of tokens seen after epoch 1 (Step: 125)\n",
            "Train loss after epoch 1 (Step: 130): 0.450\n",
            "Val loss after epoch 1 (Step: 130): 0.703\n",
            "Number of tokens seen after epoch 1 (Step: 130)\n",
            "Train loss after epoch 1 (Step: 135): 0.413\n",
            "Val loss after epoch 1 (Step: 135): 0.691\n",
            "Number of tokens seen after epoch 1 (Step: 135)\n",
            "Train loss after epoch 1 (Step: 140): 0.418\n",
            "Val loss after epoch 1 (Step: 140): 0.687\n",
            "Number of tokens seen after epoch 1 (Step: 140)\n",
            "Train loss after epoch 1 (Step: 145): 0.382\n",
            "Val loss after epoch 1 (Step: 145): 0.689\n",
            "Number of tokens seen after epoch 1 (Step: 145)\n",
            "Train loss after epoch 1 (Step: 150): 0.398\n",
            "Val loss after epoch 1 (Step: 150): 0.682\n",
            "Number of tokens seen after epoch 1 (Step: 150)\n",
            "Train loss after epoch 1 (Step: 155): 0.428\n",
            "Val loss after epoch 1 (Step: 155): 0.682\n",
            "Number of tokens seen after epoch 1 (Step: 155)\n",
            "Train loss after epoch 1 (Step: 160): 0.420\n",
            "Val loss after epoch 1 (Step: 160): 0.692\n",
            "Number of tokens seen after epoch 1 (Step: 160)\n",
            "Train loss after epoch 1 (Step: 165): 0.400\n",
            "Val loss after epoch 1 (Step: 165): 0.687\n",
            "Number of tokens seen after epoch 1 (Step: 165)\n",
            "Train loss after epoch 1 (Step: 170): 0.340\n",
            "Val loss after epoch 1 (Step: 170): 0.692\n",
            "Number of tokens seen after epoch 1 (Step: 170)\n",
            "Train loss after epoch 1 (Step: 175): 0.356\n",
            "Val loss after epoch 1 (Step: 175): 0.682\n",
            "Number of tokens seen after epoch 1 (Step: 175)\n",
            "Train loss after epoch 1 (Step: 180): 0.411\n",
            "Val loss after epoch 1 (Step: 180): 0.670\n",
            "Number of tokens seen after epoch 1 (Step: 180)\n",
            "Train loss after epoch 1 (Step: 185): 0.435\n",
            "Val loss after epoch 1 (Step: 185): 0.671\n",
            "Number of tokens seen after epoch 1 (Step: 185)\n",
            "Train loss after epoch 1 (Step: 190): 0.340\n",
            "Val loss after epoch 1 (Step: 190): 0.656\n",
            "Number of tokens seen after epoch 1 (Step: 190)\n",
            "Train loss after epoch 1 (Step: 195): 0.338\n",
            "Val loss after epoch 1 (Step: 195): 0.639\n",
            "Number of tokens seen after epoch 1 (Step: 195)\n",
            "Train loss after epoch 1 (Step: 200): 0.323\n",
            "Val loss after epoch 1 (Step: 200): 0.635\n",
            "Number of tokens seen after epoch 1 (Step: 200)\n",
            "Train loss after epoch 1 (Step: 205): 0.368\n",
            "Val loss after epoch 1 (Step: 205): 0.632\n",
            "Number of tokens seen after epoch 1 (Step: 205)\n",
            "Train loss after epoch 1 (Step: 210): 0.387\n",
            "Val loss after epoch 1 (Step: 210): 0.629\n",
            "Number of tokens seen after epoch 1 (Step: 210)\n",
            "Train loss after epoch 1 (Step: 215): 0.411\n",
            "Val loss after epoch 1 (Step: 215): 0.631\n",
            "Number of tokens seen after epoch 1 (Step: 215)\n",
            "Train loss after epoch 1 (Step: 220): 0.311\n",
            "Val loss after epoch 1 (Step: 220): 0.639\n",
            "Number of tokens seen after epoch 1 (Step: 220)\n",
            "Train loss after epoch 1 (Step: 225): 0.357\n",
            "Val loss after epoch 1 (Step: 225): 0.644\n",
            "Number of tokens seen after epoch 1 (Step: 225)\n",
            "Train loss after epoch 1 (Step: 230): 0.314\n",
            "Val loss after epoch 1 (Step: 230): 0.645\n",
            "Number of tokens seen after epoch 1 (Step: 230)\n",
            "Below is the instruction that describes a task. Write an appropriate response to address the instruction.   ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is cooked every day by the chef.<|endoftext|>The following is the instruction that describes a task. Write an appropriate response to address the instruction.   ### Response: Edit the sentence to remove any redundant\n",
            "Training completed in 2.7708035071690875 mins\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_tensor = torch.linspace(0, epochs, len(train_losses))\n",
        "plot_losses(\n",
        "    epochs_tensor, track_tokens_seen,\n",
        "    train_losses,\n",
        "    val_losses\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "l5ooXx8h7T88",
        "outputId": "1e357db7-9d5c-43f2-f2e7-442863721b8a"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lets test the fine-tuned model on test data\n",
        "for entry in test_data[:3]:\n",
        "  formatted_entry = format_entry(entry)\n",
        "  output_tokens = generate(\n",
        "      model=model,\n",
        "      inp_tokens=text_to_token_ids(formatted_entry, tokenizer).to(device),\n",
        "      context_size=BASE_CONFIG['context_length'],\n",
        "      max_tokens=256,\n",
        "      eos_id=50256\n",
        "  )\n",
        "  output_text = token_ids_to_text(output_tokens, tokenizer)\n",
        "  response_text = output_text[len(formatted_entry):].replace(\"### Response:\",'').strip()\n",
        "  print(formatted_entry)\n",
        "  print(f\"Correct Response: {entry['output']}\")\n",
        "  print(f'Model Response: {response_text}\\n\\n\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRq-6ZqoMYLe",
        "outputId": "25a91afb-0e6d-43f1-a672-dd03250dd729"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is the instruction that describes a task. Write an appropriate response to address the instruction. \n",
            "\n",
            "### Instruction:\n",
            "Rewrite the sentence using a simile.\n",
            "\n",
            "### Input:\n",
            "The car is very fast.\n",
            "Correct Response: The car is as fast as lightning.\n",
            "Model Response: The car is as fast as an elephant.\n",
            "\n",
            "\n",
            "\n",
            "Below is the instruction that describes a task. Write an appropriate response to address the instruction. \n",
            "\n",
            "### Instruction:\n",
            "What type of cloud is typically associated with thunderstorms?\n",
            "Correct Response: The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
            "Model Response: The type of cloud typically associated with thunderstorms is a cumulus.\n",
            "\n",
            "\n",
            "\n",
            "Below is the instruction that describes a task. Write an appropriate response to address the instruction. \n",
            "\n",
            "### Instruction:\n",
            "Name the author of 'Pride and Prejudice'.\n",
            "Correct Response: Jane Austen.\n",
            "Model Response: The author of 'Pride and Prejudice' is Jane Austen.\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_and_save_responses(data, model, device, tokenizer, destination_path):\n",
        "  for i, entry in tqdm(enumerate(data), total=len(data)):\n",
        "    formatted_entry = format_entry(entry)\n",
        "    output_tokens = generate(\n",
        "        model=model,\n",
        "        inp_tokens=text_to_token_ids(formatted_entry, tokenizer).to(device),\n",
        "        context_size=BASE_CONFIG['context_length'],\n",
        "        max_tokens=256,\n",
        "        eos_id=50256\n",
        "    )\n",
        "    output_text = token_ids_to_text(output_tokens, tokenizer)\n",
        "    response_text = output_text[len(formatted_entry):].replace(\"### Response:\",'').strip()\n",
        "\n",
        "    data[i]['model_response'] = response_text\n",
        "\n",
        "  with open(destination_path,'w') as file:\n",
        "    json.dump(data, file, indent=4)"
      ],
      "metadata": {
        "id": "RRTdoBr-MaG7"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "destination_path = f'/content/drive/My Drive/LLM/Data/instruction-data-with-response-lora-rank{rank}.json'\n",
        "extract_and_save_responses(test_data, model, device, tokenizer, destination_path)"
      ],
      "metadata": {
        "id": "9mJG95WIMim4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "786cdf27-cf0b-4050-de21-e55316dd5336"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 110/110 [01:46<00:00,  1.04it/s]\n"
          ]
        }
      ]
    }
  ]
}